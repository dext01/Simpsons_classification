import sys
import os
import argparse
import torch
import torch.nn as nn
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, \
    f1_score
import matplotlib.pyplot as plt
import seaborn as sns

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.dataset import get_dataloaders
from src.model import get_resnet18_finetune
from src.utils import seed_everything

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", type=str, required=True, help="–ü—É—Ç—å –∫ –≤–µ—Å–∞–º")
    parser.add_argument("--data_path", type=str, default="./data")
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--save_cm", type=str, default=None, help="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫ –≤ —Ñ–∞–π–ª (–ø—É—Ç—å)")
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    seed_everything(args.seed)

    if not os.path.exists(args.model_path):
        print(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.model_path}")
        return

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üöÄ Using device: {device}")
    print(f"‚öôÔ∏è Config: seed={args.seed}, batch_size={args.batch_size}")

    _, val_loader, classes = get_dataloaders(args.data_path, args.batch_size)
    num_classes = len(classes)

    print(f"üìä Classes: {num_classes}")
    print(f"üì¶ Val batches: {len(val_loader)}")

    # –º–æ–¥–µ–ª—å
    model = get_resnet18_finetune(num_classes=num_classes).to(device)

    try:
        state_dict = torch.load(args.model_path, map_location=device, weights_only=True)
        model.load_state_dict(state_dict)
        print(f"‚úÖ Model loaded: {args.model_path}")
    except Exception as e:
        print(f"‚ùå Error loading weights: {e}")
        return

    model.eval()
    criterion = nn.CrossEntropyLoss() # —Ñ-–∏—è –ø–æ—Ç–µ—Ä—Ä—å

    all_preds = [] # –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    all_labels = [] # –∏—Å—Ç–∏–Ω–Ω—ã–µ
    total_loss = 0.0 # –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –ª–æ—Å—Å–∞
    total_samples = 0 # —Å—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –æ–±—å–µ–∫—Ç–æ–≤

    print("üîÑ Running validation...")

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            total_loss += loss.item() * inputs.size(0)

            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            total_samples += labels.size(0)

    y_true = np.array(all_labels)
    y_pred = np.array(all_preds)
    avg_loss = total_loss / total_samples

    acc = accuracy_score(y_true, y_pred)

    prec_macro = precision_score(y_true, y_pred, average='macro')
    rec_macro = recall_score(y_true, y_pred, average='macro')
    f1_macro = f1_score(y_true, y_pred, average='macro')

    prec_weighted = precision_score(y_true, y_pred, average='weighted')
    rec_weighted = recall_score(y_true, y_pred, average='weighted')
    f1_weighted = f1_score(y_true, y_pred, average='weighted')

    print("\n" + "=" * 50)
    print("üìà –û–ë–©–ò–ï –ú–ï–¢–†–ò–ö–ò")
    print("=" * 50)
    print(f"Total Samples: {total_samples}")
    print(f"Average Loss:  {avg_loss:.4f}")
    print(f"Accuracy:      {acc * 100:.2f}%")
    print("-" * 50)
    print("Macro Average (–≤—Å–µ –∫–ª–∞—Å—Å—ã —Ä–∞–≤–Ω—ã):")
    print(f"  Precision:   {prec_macro:.4f}")
    print(f"  Recall:      {rec_macro:.4f}")
    print(f"  F1-Score:    {f1_macro:.4f}")
    print("-" * 50)
    print("Weighted Average (—É—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ –∫–ª–∞—Å—Å–∞):")
    print(f"  Precision:   {prec_weighted:.4f}")
    print(f"  Recall:      {rec_weighted:.4f}")
    print(f"  F1-Score:    {f1_weighted:.4f}")
    print("=" * 50)

    print("\nüìã –ü–û–î–†–û–ë–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–õ–ê–°–°–ê–ú:")
    labels = list(range(len(classes)))
    print(classification_report(y_true, y_pred, target_names=classes, labels=labels, digits=4, zero_division=0))
    cm = confusion_matrix(y_true, y_pred)

    print("\nüî• Confusion Matrix (–ø–µ—Ä–≤—ã–µ 10 –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏):")
    print(cm[:10, :10])

    save_path = args.save_cm or "artifacts/confusion_matrix.png"
    os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else ".", exist_ok=True)

    plt.figure(figsize=(15, 15))
    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    sns.heatmap(cm_norm, annot=False, fmt='.2f', cmap='Blues',
                xticklabels=classes, yticklabels=classes)
    plt.title("Normalized Confusion Matrix")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig(save_path)
    print(f"\n‚úÖ Confusion Matrix saved to: {save_path}")


if __name__ == "__main__":
    main()